# Task 08 â€“ Bias Detection in LLM-Generated Narratives

## ğŸ“Œ Objective
This project investigates whether large language models (LLMs) like GPT-4 and Claude demonstrate measurable bias in their interpretations of identical sports statistics when prompted with different framing or demographic cues.

We build upon the player statistics explored in Task 05 and use them to test multiple hypotheses around framing bias, demographic bias, cherry-picking of stats, and confirmation bias in AI-generated responses.

---

## ğŸ§ª Hypotheses

1. **Framing Bias â€“ Positive vs Negative Language**
   > LLMs will describe the same player more negatively when the prompt uses terms like â€œstrugglingâ€ vs â€œdevelopingâ€.

2. **Demographic Bias â€“ Seniority Mention**
   > Mentioning a player is a â€œseniorâ€ versus â€œfreshmanâ€ will result in different tone or recommendation strength.

3. **Statistical Cherry-Picking Bias**
   > Asking about strengths vs weaknesses will shift the stats that are emphasized.

4. **Gender Framing Bias**
   > Explicitly referencing the player as a â€œfemale athleteâ€ may alter tone or vocabulary in feedback.

5. **Confirmation Bias â€“ Leading vs Neutral Prompt**
   > A leading prompt like â€œWhy is this player underperforming?â€ will lead to more negative conclusions than a neutral performance review.

---

## ğŸ“ Folder Structure (Phase 1 Only)

```plaintext
Task_08_Bias_Detection/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Player_Stats_2025.csv
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ framing_bias_prompts.md
â”‚   â”œâ”€â”€ demographic_bias_prompts.md
â”‚   â”œâ”€â”€ stat_focus_prompts.md
â”‚   â”œâ”€â”€ gender_bias_prompts.md
â”‚   â””â”€â”€ confirmation_bias_prompts.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment_design.py  # Optional
â”œâ”€â”€ README.md

