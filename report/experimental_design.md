# üß™ Experimental Design ‚Äî Task 08 (Bias Detection in LLM Narratives)

## üéØ Objective

To detect and analyze the presence of **biases in LLM-generated narratives** about Syracuse Women‚Äôs Lacrosse players, based on how we phrase the prompt.  
We specifically test whether changes in:

- **Framing** (positive vs negative wording)  
- **Confirmation** (leading vs open-ended questions)  
- **Gender mention** (with vs without ‚Äúfemale‚Äù language)  
- **Experience / role labels** (senior vs freshman, midfield vs defense)  
- **Statistical emphasis** (strength vs weakness focus)

lead to systematically different model responses when the **underlying 2025 stat lines are held constant**.

---

## üß† Hypothesis

We hypothesize that **biased or loaded prompts** (e.g., negative framing, gender-coded wording, or questions that presuppose failure) will systematically produce:

- more **critical or pessimistic tone**,  
- more **judgmental language**, and  
- more **focus on weaknesses or discipline issues**

compared to more neutral, balanced prompts that describe the **same stat profile**.

---

## üß© Bias Dimensions Tested

| Bias Type         | What We Manipulate (A vs B)                                                  | What Stays Constant                                   |
|-------------------|-------------------------------------------------------------------------------|-------------------------------------------------------|
| Framing Bias      | Negative vs positive wording around the same stat line (e.g., ‚Äústat-padding attacker‚Äù vs ‚Äúelite playmaker‚Äù). | Player, games, goals, assists, shots, fouls.          |
| Confirmation Bias | Leading question that presupposes under-performance vs neutral ‚Äúhow would you assess‚Ä¶?‚Äù question. | Exact 2025 stat line for that player.                 |
| Gender Bias       | Prompts that explicitly say **‚Äúfemale athlete‚Äù** vs prompts that just say **‚Äúthis player‚Äù**. | Stat line and generic role.                           |
| Demographic Bias* | **Experience / role labels only**: senior vs freshman; midfielder vs defender. | Same combined stat line in each pair.                 |
| Stat Focus Bias   | Prompts that highlight strengths vs prompts that highlight weaknesses; positive vs negative interpretive angle. | Full stat line; we only change which parts we talk about. |

\*For this task, **‚Äúdemographic‚Äù is intentionally limited to non-sensitive team labels** (seniority, on-field role). We do **not** use race, ethnicity, or other protected attributes.

---

## üßæ Prompt Format

All prompts live in the `/prompts/` folder and are grouped by bias type:

- `framing_bias_prompts.md`  
- `confirmation_bias_prompts.md`  
- `gender_bias_prompts.md`  
- `demographic_bias_prompts.md`  
- `stat_focus_prompts.md`

Within each file we define **prompt pairs**:

- **Prompt A** ‚Äî the *biased* or more loaded variant  
- **Prompt B** ‚Äî the *neutral* or more balanced version  

Each pair:

- is tied to a **concrete 2025 stat line** (e.g., Emma Ward‚Äôs goals / assists / shots, Kaci Benoit‚Äôs GB / CT / fouls, etc.),  
- keeps that stat line **identical between A and B**, and  
- changes **only one dimension of wording** (framing, leading vs neutral, gender mention, etc.).

Some prompts include player names (e.g., *Emma Ward*, *Meghan Rode*) when that helps anchor the narrative; others say ‚Äúthis player‚Äù to focus purely on the label being manipulated.

---

## üéõÔ∏è Prompt Delivery Plan

Prompt execution is fully scripted via `run_experiments.py`:

1. **Model Setup**
   - **GPT-4o** via the OpenAI API (`OPENAI_API_KEY` from `.env`).  
   - **Llama-4 Maverick 17B Instruct** via Groq (`GROQ_API_KEY` from `.env`).

2. **Execution Loop**
   - For each `*_bias_prompts.md` file in `/prompts/`:
     - Parse all **Prompt A / Prompt B pairs**.
     - For each pair, call **both models** with Prompt A and then Prompt B independently.
   - Save all responses to `/results/{bias_type}_results.json`.

   - Prompts are executed programmatically using `run_experiments.py`.
   - For each prompt pair (A/B), we call:
     - **GPT-4o** via the OpenAI API (primary model for analysis).
     - **LLaMA 4 Maverick Instruct** via the Groq API (secondary model for comparison), where available.
   - Model responses are saved as structured JSON files under the `/results/` folder, one file per bias type.

3. **Output Structure**  
   Each JSON result file is a list of records:

   ```json
   {
     "model": "gpt-4o or llama-4-‚Ä¶",
     "prompt_pair_id": 1,
     "prompt_a": "...full text...",
     "response_a": "...model output...",
     "prompt_b": "...full text...",
     "response_b": "...model output..."
   }
